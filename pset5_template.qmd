---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")

import pandas as pd
import requests
from bs4 import BeautifulSoup
import urllib3
import time
```

## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)
1. Scraping: Go to the first page of the HHS OIG’s “Enforcement Actions” page and scrape and collect the following into a dataset:

• Title of the enforcement action
• Date
• Category (e.g, “Criminal and Civil Actions”)
• Link associated with the enforcement action

Collect your output into a tidy dataframe and print its head.

```{python}
url = "https://oig.hhs.gov/fraud/enforcement/"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")
```

```{python}
enforcement_data = []
enforcement_cards = soup.find_all('li', class_='usa-card')

for card in enforcement_cards:
    title_tag = card.find('h2', class_='usa-card__heading')
    title = title_tag.find('a').get_text(strip=True) if title_tag else 'N/A'

    date_tag = card.find('span', class_='text-base-dark')
    date = date_tag.get_text(strip=True) if date_tag else 'N/A'

    category_tag = card.find('li', class_='usa-tag')
    category = category_tag.get_text(strip=True) if category_tag else 'N/A'

    link_tag = title_tag.find('a') if title_tag else None
    link = link_tag['href'] if link_tag else 'N/A'
    full_link = f"https://oig.hhs.gov{link}"

    enforcement_data.append({
        'Title of Enforcement Action': title,
        'Date': date,
        'Category': category,
        'Associated Link': full_link
    })
    time.sleep(0.2)

OIG_df = pd.DataFrame(enforcement_data)
print(OIG_df.head())
```

### 2. Crawling (PARTNER 1)
2. Crawling: Then for each enforcement action, click the link and collect the name of the agency involved (e.g., for this link, it would be U.S. Attorney’s Oﬀice, Eastern District of Washington). Update your dataframe with the name of the agency and print its head.

Hint: if you go to James A. Robinson’s profile page at the Nobel Prize website here, right click anywhere along the line “Aﬀiliation at the time of the award: University of Chicago, Chicago, IL, USA”, and select Inspect, you’ll see that this aﬀiliation information is located at the third <p> tag out of five <p> tags under the <div class="content">. Think about how you can select the third element of <p> out of five <p> elements so you’re sure you scrape the aﬀiliation information, not other. This way, you can scrape the name of agency to answer this question.

```{python}
import requests
from bs4 import BeautifulSoup
import time
import pandas as pd

# Base URL for the site
base_url = "https://oig.hhs.gov"

# Main page URL where enforcement actions are listed
url = "https://oig.hhs.gov/fraud/enforcement/"

# Send request to the main page and parse the HTML content
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Step 1: Extract links to individual enforcement action pages
# We need to find all <a> tags where the 'href' contains 'fraud/enforcement/'
enforcement_links = []

for link in soup.find_all('a', href=True):
    href = link['href']
    # Only include links that contain 'fraud/enforcement/'
    if 'fraud/enforcement/' in href:
        full_link = f"{base_url}{href}"
        enforcement_links.append(full_link)

# Step 2: Follow each link and extract the agency name
enforcement_data = []

for link in enforcement_links:
    # Send a GET request to each enforcement page
    action_response = requests.get(link)
    action_soup = BeautifulSoup(action_response.content, "html.parser")
    
    # Step 3: Extract the agency name from the enforcement action page
    # Look for the <li> containing the text 'Agency:'
    agency_tag = action_soup.find('li', string=lambda text: text and 'Agency:' in text)
    agency = agency_tag.get_text(strip=True).replace('Agency:', '').strip() if agency_tag else 'N/A'
    
    # Step 4: Collect the data (Agency name and associated link)
    enforcement_data.append({
        'Agency': agency,
        'Link': link
    })
    
    # To avoid overwhelming the server, add a small delay
    time.sleep(1)

# Step 5: Create a DataFrame to hold the data
OIG_df = pd.DataFrame(enforcement_data)

# Step 6: Display the DataFrame
print(OIG_df.head())  # Print the first few rows



```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)
#define function
def year-scraper(data, month, year):
  for action in data:
    if year >= 2013:
    
      time.sleep(4) #will cause the scraper to delay for 4 seconds
    else:
      print("Only search for years in 2013 and later")

* b. Create Dynamic Scraper (PARTNER 2)

```{python}

```

* c. Test Partner's Code (PARTNER 1)

```{python}

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```
