---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")

import pandas as pd
import requests
from bs4 import BeautifulSoup
import urllib3
import time
```

## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)
1. Scraping: Go to the first page of the HHS OIG’s “Enforcement Actions” page and scrape and collect the following into a dataset:

• Title of the enforcement action
• Date
• Category (e.g, “Criminal and Civil Actions”)
• Link associated with the enforcement action

Collect your output into a tidy dataframe and print its head.

```{python}
url = "https://oig.hhs.gov/fraud/enforcement/"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")
```

```{python}
enforcement_data = []
enforcement_cards = soup.find_all('li', class_='usa-card')

for card in enforcement_cards:
    title_tag = card.find('h2', class_='usa-card__heading')
    title = title_tag.find('a').get_text(strip=True) if title_tag else 'N/A'

    date_tag = card.find('span', class_='text-base-dark')
    date = date_tag.get_text(strip=True) if date_tag else 'N/A'

    category_tag = card.find('li', class_='usa-tag')
    category = category_tag.get_text(strip=True) if category_tag else 'N/A'

    link_tag = title_tag.find('a') if title_tag else None
    link = link_tag['href'] if link_tag else 'N/A'
    full_link = f"https://oig.hhs.gov{link}"

    enforcement_data.append({
        'Title of Enforcement Action': title,
        'Date': date,
        'Category': category,
        'Associated Link': full_link
    })
    time.sleep(0.2)

OIG_df = pd.DataFrame(enforcement_data)
print(OIG_df.head())
```

### 2. Crawling (PARTNER 1)
2. Crawling: Then for each enforcement action, click the link and collect the name of the agency involved (e.g., for this link, it would be U.S. Attorney’s Oﬀice, Eastern District of Washington). Update your dataframe with the name of the agency and print its head.

Hint: if you go to James A. Robinson’s profile page at the Nobel Prize website here, right click anywhere along the line “Aﬀiliation at the time of the award: University of Chicago, Chicago, IL, USA”, and select Inspect, you’ll see that this aﬀiliation information is located at the third <p> tag out of five <p> tags under the <div class="content">. Think about how you can select the third element of <p> out of five <p> elements so you’re sure you scrape the aﬀiliation information, not other. This way, you can scrape the name of agency to answer this question.

```{python}
import requests
from lxml import html

def extract_agency_from_page(url):
    try:
        response = requests.get(url)
        if response.status_code != 200:
            return 'Failed to retrieve page'
        
        tree = html.fromstring(response.content)

        agency_text = tree.xpath('/html/body/main/div/div[2]/article/div/ul/li[2]/text()')

        if agency_text:
            return agency_text[0].strip()
        else:
            return 'No Agency Info'
    
    except Exception as e:
        print(f"Error processing {url}: {e}")
        return 'Error Fetching Agency'
```

```{python}
import time

def add_agency_column_to_OIG(OIG_df):
    agency_names = []

    for index, row in OIG_df.iterrows():
        link = row['Associated Link']
        if link:
            print(f"Processing link {index + 1}: {link}")
            agency_name = extract_agency_from_page(link)
        else:
            agency_name = 'No Link Available'
        agency_names.append(agency_name)
        time.sleep(1)

    OIG_df['Agency'] = agency_names
    return OIG_df

OIG_df = add_agency_column_to_OIG(OIG_df)
print(OIG_df.head())
```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)


#datetime will be required for conditioning on year_month
from datetime import date

#receive input of web link and
#a date object called year_month 
def year_scraper(site, year_month):
  #receiving object
  enforcement_actions_year_month = []

  #first run a function to read the data in
 
  #then run enforced_frame()

  #then run for loop with condition

  #create date object for January 2nd, 2013
  for action in data:
    if data["Date"] >= year_month:
      
      time.sleep(1) #will cause the scraper to delay for 2 seconds
      yr_month_actions.append(action)
    else:
      print("Only search for years in 2013 and later!")
  return(yr_month_actions.to_csv("enforcement_actions_year_month.csv"))

# create each chunk separately, and then put them together



Take the data and select Title of enforcement action, date, category, and the link to the enforcement action

```{python}
import datetime as datetime
from datetime import datetime
from datetime import date

def enforce_frame(soup):
  #container to be turned into df
  enforcement_data = []
  #capture all 'li' tag contents
  enforcement_cards = soup.find_all('li', class_='usa-card')
  #iterate through enforcement_cards for the 4 relevant tags
  for card in enforcement_cards:
      title_tag = card.find('h2', class_='usa-card__heading')
      title = title_tag.find('a').get_text(strip=True) if title_tag else 'N/A'

      date_tag = card.find('span', class_='text-base-dark')
      date = date_tag.get_text(strip=True) if date_tag else 'N/A'

      category_tag = card.find('li', class_='usa-tag')
      category = category_tag.get_text(strip=True) if category_tag else 'N/A'

      link_tag = title_tag.find('a') if title_tag else None
      link = link_tag['href'] if link_tag else 'N/A'
      full_link = f"https://oig.hhs.gov{link}"

      enforcement_data.append({
        'Title of Enforcement Action': title,
        'Date': date,
        'Category': category,
        'Associated Link': full_link
    })
      time.sleep(0.2)

  OIG_df = pd.DataFrame(enforcement_data)
  return(OIG_df)


```

#select the enforcement links and iterate through each
```{python}
# Base URL for the site
# this will be required for the function
# define this before running the function
base_url = "https://oig.hhs.gov"

def add_agency_column_to_OIG(OIG_df):
    agency_names = []

    for index, row in OIG_df.iterrows():
        link = row['Associated Link']
        if link:
            print(f"Processing link {index + 1}: {link}")
            agency_name = extract_agency_from_page(link)
        else:
            agency_name = 'No Link Available'
        agency_names.append(agency_name)
        time.sleep(1)
    OIG_df['Agency'] = agency_names
    return OIG_df
OIG_df = add_agency_column_to_OIG(OIG_df)
print(OIG_df.head())

```

#create a loop to loop through the "Date" category
```{python}
import datetime as datetime
from datetime import datetime
from datetime import date

#set year_month as the first month of 2013
target_date = date(2013, 1, 1)

#site will be the website we've been using
site = "https://oig.hhs.gov/fraud/enforcement/"

# Base URL for the site 
# (will be required for get_links() function)
base_url = "https://oig.hhs.gov"

#adding code from Step 1
#make it a function for easier manipulation
def reading_data(url): #url will be the appropriate link
  response = requests.get(url)
  soup = BeautifulSoup(response.content, "html.parser")
  return(soup)

#code for selecting specifically date tags and saving them
def find_dates(soup):
  date_list = []
  enforcement_cards = soup.find_all('li', class_='usa-card')
  for card in enforcement_cards:
    date_tag = card.find('span', class_='text-base-dark')
    the_date = date_tag.get_text(strip=True) if date_tag else 'N/A'
    my_date = datetime.strptime(the_date, "%B %d, %Y")
    date_list.append(my_date)
  return(date_list)

#code for moving between pages based on date tags
def site_moving(site):
    #define the dataframe that will receive
    #all info
    full_data = pd.DataFrame(columns = ['Title of Enforcement Action', 'Date', 'Category', 'Associated Link', 'Agency'])
    current_url = site
    while current_url:
        #read in data
        soup = reading_data(current_url)
        #create list of dates
        date_list = find_dates(soup)
        #fill dataframe 
        df = enforce_frame(soup)
        #add "Agents" to dataframe
        OIG_df = add_agency_column_to_OIG(df)
        #concat data
        full_data = pd.concat([full_data, OIG_df], ignore_index=True)

        # Check if any date on this page is before the target date
        if any(d.date() < target_date for d in date_list):
            print("Stopping, found a date before target:", target_date)
            break
        
        # Find next page link
        next_link_tag = soup.find("a", class_="pagination-next")
        if next_link_tag:
            next_link = next_link_tag.get("href")
            current_url = f"{site}{next_link}"
            
        else:
            print("No more pages to navigate.")
            break
    return full_data

#each time you move to a new page, make a date_list
#if no object in that date list is less than 2023-1
#find the next page tab and request that link,
#put that link in reading_data()


 
```

Attribution: Asked ChatGPT how to create datetime object that would capture both year and month at once.  Asked ChatGPT how to convert an object into a datetime object. Also asked ChatGPT how to create a function that would loop in order to move forward through pages. The main change was switching to a while loop

#Put everything together
```{python}
import datetime as datetime
from datetime import datetime
from datetime import date

#set year_month as the first month of 2013
target_date = date(2023, 1, 1)
#site will be the website we've been using
site = "https://oig.hhs.gov/fraud/enforcement/"
# Base URL for the site 
# (will be required for get_links() function)
base_url = "https://oig.hhs.gov"

def the_crawler(site, target_date, base_url):
  #read in the data
  soup = reading_data(site)
  #check whether your year is greater than 
  #or equal to 2013
  if target_date <= date(2013, 1, 1):
    print("Turn back! Subset for 2013 and later")
  else:
    #insert the function for scraping
    all_the_data = site_moving(site)
  return all_the_data.to_csv("enforcement_actions_year_month.csv", index = False)

the_crawler(site, target_date, base_url)
```

* c. Test Partner's Code (PARTNER 1)

```{python}
import datetime as datetime
from datetime import datetime
from datetime import date

#set year_month as the first month of 2013
target_date = date(2021, 1, 1)
#site will be the website we've been using
site = "https://oig.hhs.gov/fraud/enforcement/"
# Base URL for the site 
# (will be required for get_links() function)
base_url = "https://oig.hhs.gov"

def the_crawler(site, target_date, base_url):
  #read in the data
  soup = reading_data(site)
  #check whether your year is greater than 
  #or equal to 2013
  if target_date <= date(2013, 1, 1):
    print("Turn back! Subset for 2013 and later")
  else:
    #insert the function for scraping
    all_the_data = site_moving(site)
  return all_the_data.to_csv("enforcement_actions_year_month.csv", index = False)

the_crawler(site, target_date, base_url)
```

```{python}
enforcement_actions_2021 = pd.read_csv("/Users/justinesilverstein/Desktop/Andrew_and_Justine_Pset5/enforcement_actions_year_month.csv")
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

Plot a line chart that shows: the number of enforcement actions split out by:

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}
enforcement_actions_2021['Date'] = pd.to_datetime(enforcement_actions_2021['Date'], errors='coerce')

category_filter = enforcement_actions_2021[enforcement_actions_2021['Category'].isin(['Criminal and Civil Actions', 'State Enforcement Agencies'])]
category_counts = category_filter.groupby([category_filter['Date'].dt.to_period('M'), 'Category']).size().reset_index(name='Count')
category_counts['Date'] = category_counts['Date'].dt.to_timestamp()

line_chart = alt.Chart(category_counts).mark_line().encode(
    x='Date:T',  
    y='Count:Q',  
    color='Category:N',  
    tooltip=['Date:T', 'Category:N', 'Count:Q']  
).properties(
    title="Enforcement Actions by Category Over Time"
)

line_chart.show()
```

* based on five topics

Hint:
You will need to divide the five topics manually by looking at the title and assigning the relevant topic. For example, if you find the word “bank” or “financial” in the title of an action, then that action should probably belong to “Financial Fraud” topic.

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)
Map by state: Among actions taken by state-level agencies, clean the state names you collected and plot a choropleth of the number of enforcement actions for each state. Hint: look for “State of” in the agency info!

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```
