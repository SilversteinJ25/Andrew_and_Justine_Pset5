---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")

import pandas as pd
import requests
from bs4 import BeautifulSoup
import urllib3
import time
```

## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)
1. Scraping: Go to the first page of the HHS OIG’s “Enforcement Actions” page and scrape and collect the following into a dataset:

• Title of the enforcement action
• Date
• Category (e.g, “Criminal and Civil Actions”)
• Link associated with the enforcement action

Collect your output into a tidy dataframe and print its head.

```{python}
url = "https://oig.hhs.gov/fraud/enforcement/"
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")
```

```{python}
enforcement_data = []
enforcement_cards = soup.find_all('li', class_='usa-card')

for card in enforcement_cards:
    title_tag = card.find('h2', class_='usa-card__heading')
    title = title_tag.find('a').get_text(strip=True) if title_tag else 'N/A'

    date_tag = card.find('span', class_='text-base-dark')
    date = date_tag.get_text(strip=True) if date_tag else 'N/A'

    category_tag = card.find('li', class_='usa-tag')
    category = category_tag.get_text(strip=True) if category_tag else 'N/A'

    link_tag = title_tag.find('a') if title_tag else None
    link = link_tag['href'] if link_tag else 'N/A'
    full_link = f"https://oig.hhs.gov{link}"

    enforcement_data.append({
        'Title of Enforcement Action': title,
        'Date': date,
        'Category': category,
        'Associated Link': full_link
    })
    time.sleep(0.2)

OIG_df = pd.DataFrame(enforcement_data)
print(OIG_df.head())
```

### 2. Crawling (PARTNER 1)
2. Crawling: Then for each enforcement action, click the link and collect the name of the agency involved (e.g., for this link, it would be U.S. Attorney’s Oﬀice, Eastern District of Washington). Update your dataframe with the name of the agency and print its head.

Hint: if you go to James A. Robinson’s profile page at the Nobel Prize website here, right click anywhere along the line “Aﬀiliation at the time of the award: University of Chicago, Chicago, IL, USA”, and select Inspect, you’ll see that this aﬀiliation information is located at the third <p> tag out of five <p> tags under the <div class="content">. Think about how you can select the third element of <p> out of five <p> elements so you’re sure you scrape the aﬀiliation information, not other. This way, you can scrape the name of agency to answer this question.

```{python}
import requests
from bs4 import BeautifulSoup
import time
import pandas as pd

# Base URL for the site
base_url = "https://oig.hhs.gov"

# Main page URL where enforcement actions are listed
url = "https://oig.hhs.gov/fraud/enforcement/"

# Send request to the main page and parse the HTML content
response = requests.get(url)
soup = BeautifulSoup(response.content, "html.parser")

# Step 1: Extract links to individual enforcement action pages
# We need to find all <a> tags where the 'href' contains 'fraud/enforcement/'
enforcement_links = []

for link in soup.find_all('a', href=True):
    href = link['href']
    # Only include links that contain 'fraud/enforcement/'
    if 'fraud/enforcement/' in href:
        full_link = f"{base_url}{href}"
        enforcement_links.append(full_link)

# Step 2: Follow each link and extract the agency name
enforcement_data = []

for link in enforcement_links:
    # Send a GET request to each enforcement page
    action_response = requests.get(link)
    action_soup = BeautifulSoup(action_response.content, "html.parser")
    
    # Step 3: Extract the agency name from the enforcement action page
    # Look for the <li> containing the text 'Agency:'
    agency_tag = action_soup.find('li', string=lambda text: text and 'Agency:' in text)
    agency = agency_tag.get_text(strip=True).replace('Agency:', '').strip() if agency_tag else 'N/A'
    
    # Step 4: Collect the data (Agency name and associated link)
    enforcement_data.append({
        'Agency': agency,
        'Link': link
    })
    
    # To avoid overwhelming the server, add a small delay
    time.sleep(1)

# Step 5: Create a DataFrame to hold the data
OIG_df = pd.DataFrame(enforcement_data)

# Step 6: Display the DataFrame
print(OIG_df.head())  # Print the first few rows



```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)

#datetime will be required for conditioning on year_month
from datetime import date

#receive input of web link and
#a date object called year_month 
def year_scraper(site, year_month):
  #receiving object
  enforcement_actions_year_month = []

  #first run a function to read the data in
 
  #then run enforced_frame()

  #then run for loop with condition

  #create date object for January 2nd, 2013
  for action in data:
    if data["Date"] >= year_month:
      
      time.sleep(1) #will cause the scraper to delay for 2 seconds
      yr_month_actions.append(action)
    else:
      print("Only search for years in 2013 and later!")
  return(yr_month_actions.to_csv("enforcement_actions_year_month.csv"))

# create each chunk separately, and then put them together


Attribution: Asked ChatGPT how to convert an object into a 
datetime object. Also asked ChatGPT to trouble shoot 
my code, and received the suggestion to select href 
directly in site_moving()

Take the data and select Title of enforcement action, date, category,
and the link to the enforcement action

```{python}
import datetime as datetime
from datetime import datetime
from datetime import date

def enforce_frame(soup):
  #container to be turned into df
  enforcement_data = []
  #capture all 'li' tag contents
  enforcement_cards = soup.find_all('li', class_='usa-card')
  #iterate through enforcement_cards for the 4 relevant tags
  for card in enforcement_cards:
      title_tag = card.find('h2', class_='usa-card__heading')
      title = title_tag.find('a').get_text(strip=True) if title_tag else 'N/A'

      date_tag = card.find('span', class_='text-base-dark')
      date = date_tag.get_text(strip=True) if date_tag else 'N/A'

      category_tag = card.find('li', class_='usa-tag')
      category = category_tag.get_text(strip=True) if category_tag else 'N/A'

      link_tag = title_tag.find('a') if title_tag else None
      link = link_tag['href'] if link_tag else 'N/A'
      full_link = f"https://oig.hhs.gov{link}"

      enforcement_data.append({
        'Title of Enforcement Action': title,
        'Date': date,
        'Category': category,
        'Associated Link': full_link
    })
      time.sleep(0.2)

  OIG_df = pd.DataFrame(enforcement_data)
  #an addition to make datetime object
  OIG_df['Date'] = pd.to_datetime(OIG_df['Date'])
  return(OIG_df)


```

#select the enforcement links and iterate through each
```{python}
# Base URL for the site
# this will be required for the function
# define this before running the function
base_url = "https://oig.hhs.gov"

def get_links(data):

#use soup object made in reading_data() 
# Step 1: Extract links to individual enforcement action pages
# We need to find all <a> tags where 
# the 'href' contains 'fraud/enforcement/'
  enforcement_links = []

  for link in soup.find_all('a', href=True):
      href = link['href']
    # Only include links that contain 'fraud/enforcement/'
      if 'fraud/enforcement/' in href:
          full_link = f"{base_url}{href}"
          enforcement_links.append(full_link)

# Step 2: Follow each link and extract the agency name
  enforcement_data = []

  for link in enforcement_links:
    # Send a GET request to each enforcement page
      action_response = requests.get(link)
      action_soup = BeautifulSoup(action_response.content, "html.parser")
    
    # Step 3: Extract the agency name from the enforcement action page
    # Look for the <li> containing the text 'Agency:'
      agency_tag = action_soup.find('li', string=lambda text: text and 'Agency:' in text)
      agency = agency_tag.get_text(strip=True).replace('Agency:', '').strip() if agency_tag else 'N/A'
    
    # Step 4: Collect the data (Agency name and associated link)
      enforcement_data.append({
        'Agency': agency,
        'Link': link
      })
    
    # To avoid overwhelming the server, add a small delay
      time.sleep(1)

# Step 5: Create a DataFrame to hold the data. 
  OIG_df = pd.DataFrame(enforcement_data)
  return(OIG_df)

```

#create a loop to loop through the "Date" category
```{python}
import datetime as datetime
from datetime import datetime
from datetime import date

#set year_month as the first month of 2013
year_month = date(2013, 1, 1)

#site will be the website we've been using
site = "https://oig.hhs.gov/fraud/enforcement/"

# Base URL for the site 
# (will be required for get_links() function)
base_url = "https://oig.hhs.gov"


#adding code from Step 1
#make it a function for easier manipulation
def reading_data(url): #url will be the appropriate link
  response = requests.get(url)
  soup = BeautifulSoup(response.content, "html.parser")
  return(soup)

#code for selecting specifically date tags and saving them
def find_dates(soup):
  date_list = []
  enforcement_cards = soup.find_all('li', class_='usa-card')
  for card in enforcement_cards:
    date_tag = card.find('span', class_='text-base-dark')
    the_date = date_tag.get_text(strip=True) if date_tag else 'N/A'
    my_date = datetime.strptime(the_date, "%B %d, %Y")
    date_list.append(my_date)
  return(date_list)

base_url = "https://oig.hhs.gov"
year_month = date(2013, 1, 1)

#code for moving between pages based on date tags
def site_moving(start_url):
    current_url = start_url
    while current_url:
        soup = reading_data(current_url)
        date_list = find_dates(soup)
        OIG_df = get_links(soup)
        # Check if any date on this page is before the target date
        if any(d.date() < target_date for d in date_list):
            print("Stopping, found a date before target:", target_date)
            break
        
        # Find next page link
        next_link_tag = soup.find("a", class_="pagination-next")
        if next_link_tag:
            next_link = next_link_tag.get("href")
            current_url = f"{base_url}{next_link}"
            
        else:
            print("No more pages to navigate.")
            break

#each time you move to a new page, make a date_list
#if no object in that date list is less than 2023-1
#find the next page tab and request that link,
#put that link in reading_data()


 
```

Attribution: Asked ChatGPT how to create datetime object that would 
capture both year and month at once. 

* c. Test Partner's Code (PARTNER 1)

```{python}

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```
